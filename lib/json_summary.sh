#!/bin/bash
# lib/json_summary.sh - Versión mejorada
# Build a machine-readable JSON summary from artifacts/
num_or_zero() { case "$1" in ''|*[!0-9]*) echo 0 ;; *) echo "$1" ;; esac }
now_utc() { date -u +%Y-%m-%dT%H:%M:%SZ; }

collect_metrics_from_stdout() {
  local file="$1"
  [ -f "$file" ] || { echo "{}"; return 0; }
  awk -F'METRIC:' '/^METRIC:/{print $2}' "$file" | \
    awk -F'=' '{
      key=$1; sub(/^[ \t]+/,"",key); sub(/[ \t]+$/,"",key);
      $1=""; val=substr($0,2);
      gsub(/^[ \t]+|[ \t]+$/, "", val);
      printf("(%s)=(%s)\n", key, val);
    }' | jq -Rn '
      reduce inputs as $line ({}; ($line | capture("\\((?<k>[^)]*)\\)=\\((?<v>.*)\\)")) as $kv
        | . + {($kv.k): $kv.v})'
}

# ===== MEJORA: Parser JSON más robusto =====
parse_result_line() {
  local line="$1"
  
  # Simple parsing without complex jq operations
  echo "$line" | awk -F'|' '{
    printf "{\"status\":\"%s\",\"fields\":{", $1
    for(i=2; i<=NF; i++) {
      if($i ~ /=/) {
        split($i, kv, "=")
        if(i>2) printf ","
        printf "\"%s\":\"%s\"", kv[1], (kv[2] ? kv[2] : "")
      }
    }
    printf "}}\n"
  }'
}

# Función helper para extraer campos específicos  
extract_field() {
  local json="$1" field="$2" default="${3:-}"
  echo "$json" | jq -r ".fields.${field} // \"${default}\"" 2>/dev/null || echo "$default"
}

# Extraer políticas fallidas de manera segura
extract_failed_policies() {
  local detail="$1"
  if [[ "$detail" == *"[POLICY]"* ]]; then
    echo "$detail" | grep -oE 'name=[^ ]+' | sed 's/name=//' | head -5 | jq -Rn '[inputs]' 2>/dev/null || echo "[]"
  else
    echo "[]"
  fi
}

parse_skip_line() {
  local line="$1"
  
  echo "$line" | jq -Rn '
    input |
    split("|") as $parts |
    {
      code: $parts[0],
      fields: (
        $parts[1:] |
        map(select(length > 0) |
          if test("^(series|type|size|offer|sku|vm)=") then
            split("=") | {key: .[0], value: (.[1:] | join("="))}
          else
            {key: "message", value: .}
          end
        ) |
        from_entries
      )
    }'
}

write_summary_json() {
  local out="$1" results_log="$2" skip_log="$3" rg="$4" location="$5" series_filter="$6" max_parallel="$7"

  local totals_good totals_bad totals_skip
  totals_good=$(grep -c '^GOOD|' "$results_log" 2>/dev/null || echo 0)
  totals_bad=$(grep -c '^BAD|'  "$results_log" 2>/dev/null || echo 0)
  totals_skip=$(wc -l < "$skip_log" 2>/dev/null | tr -d ' ' || echo 0)

  # Base object con información adicional del sistema
  jq -n --arg ts "$(now_utc)" \
        --arg rg "$rg" --arg loc "$location" \
        --arg sf "$series_filter" \
        --argjson mp "$(num_or_zero "$max_parallel")" \
        --argjson good "$(num_or_zero "$totals_good")" \
        --argjson bad  "$(num_or_zero "$totals_bad")" \
        --argjson skip "$(num_or_zero "$totals_skip")" \
        --arg runner_host "$(hostname 2>/dev/null || echo unknown)" \
        --arg runner_user "$(whoami 2>/dev/null || echo unknown)" \
        --arg azure_subscription "$(az account show --query id -o tsv 2>/dev/null || echo unknown)" '
    {
      _comments: "Test summary generated by improved json_summary.sh",
      _version: "2.0",
      run: { 
        timestamp: $ts, 
        resource_group: $rg, 
        location: $loc, 
        series_filter: $sf, 
        max_parallel: $mp,
        runner_host: $runner_host,
        runner_user: $runner_user,
        azure_subscription: $azure_subscription
      },
      totals: { 
        GOOD: $good, 
        BAD: $bad, 
        SKIP: $skip,
        total: ($good + $bad + $skip),
        success_rate: (if ($good + $bad) > 0 then ($good / ($good + $bad) * 100 | floor) else 0 end)
      },
      results: [], 
      skips: [], 
      metrics: [], 
      vm_rollup: {},
      summary_stats: {}
    }' > "$out"

  # ===== MEJORA: Procesar results con parser robusto =====
  if [ -f "$results_log" ] && [ -s "$results_log" ]; then
    tmp_res="$(mktemp)"
    while IFS= read -r line; do
      # Simple JSON construction avoiding complex jq operations
      status=$(echo "$line" | cut -d'|' -f1)
      
      # Extract other fields safely
      vm=$(echo "$line" | grep -oE 'vm=[^|]*' | cut -d'=' -f2- || echo "")
      test=$(echo "$line" | grep -oE 'test=[^|]*' | cut -d'=' -f2- || echo "")
      series=$(echo "$line" | grep -oE 'series=[^|]*' | cut -d'=' -f2- || echo "")
      type=$(echo "$line" | grep -oE 'type=[^|]*' | cut -d'=' -f2- || echo "")
      size=$(echo "$line" | grep -oE 'size=[^|]*' | cut -d'=' -f2- || echo "")
      offer=$(echo "$line" | grep -oE 'offer=[^|]*' | cut -d'=' -f2- || echo "")
      sku=$(echo "$line" | grep -oE 'sku=[^|]*' | cut -d'=' -f2- || echo "")
      detail=$(echo "$line" | grep -oE 'detail=[^|]*' | cut -d'=' -f2- || echo "")
      
      # Simple JSON without complex processing
      cat << EOF
{
  "status": "$status",
  "vm": "$vm", 
  "test": "$test",
  "series": "$series",
  "type": "$type", 
  "size": "$size",
  "offer": "$offer",
  "sku": "$sku",
  "detail": "$detail",
  "failed_policies": [],
  "failure_category": "unknown",
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
}
EOF
    done < "$results_log" | jq -s '.' > "$tmp_res" 2>/dev/null || echo "[]" > "$tmp_res"
    
    jq --slurpfile arr "$tmp_res" '.results = $arr[0]' "$out" > "$out.tmp" && mv "$out.tmp" "$out"
    rm -f "$tmp_res"
  fi

  # ===== MEJORA: Procesar skips con parser robusto =====
  if [ -f "$skip_log" ] && [ -s "$skip_log" ]; then
    tmp_sk="$(mktemp)"
    while IFS= read -r line; do
      # Simple extraction without complex jq
      code=$(echo "$line" | cut -d'|' -f1)
      
      # Extract fields safely
      vm=$(echo "$line" | grep -oE 'vm=[^|]*' | cut -d'=' -f2- || echo "")
      series=$(echo "$line" | grep -oE 'series=[^|]*' | cut -d'=' -f2- || echo "")
      type=$(echo "$line" | grep -oE 'type=[^|]*' | cut -d'=' -f2- || echo "")
      size=$(echo "$line" | grep -oE 'size=[^|]*' | cut -d'=' -f2- || echo "")
      offer=$(echo "$line" | grep -oE 'offer=[^|]*' | cut -d'=' -f2- || echo "")
      sku=$(echo "$line" | grep -oE 'sku=[^|]*' | cut -d'=' -f2- || echo "")
      
      # Extract message (everything after the last field)
      message=$(echo "$line" | sed 's/^[^|]*|//g' | sed 's/[a-z]*=[^|]*|//g' | sed 's/[a-z]*=[^|]*$//g')
      
      # Classify skip category
      skip_category="other"
      case "$code" in
        PRE:*) skip_category="pre_validation" ;;
        RUN:CREATE*) skip_category="vm_creation" ;;
        RUN:IP|RUN:SSH*) skip_category="connectivity" ;;
        RUN:REBOOT*) skip_category="reboot" ;;
        RUN:ABORT*) skip_category="aborted" ;;
      esac
      
      is_critical="false"
      case "$code" in
        RUN:CREATE|RUN:CREATE_POWER|RUN:IP|RUN:SSH|RUN:SSH_LOST|RUN:REBOOT_SSH) is_critical="true" ;;
      esac
      
      cat << EOF
{
  "code": "$code",
  "vm": "$vm",
  "series": "$series", 
  "type": "$type",
  "size": "$size",
  "offer": "$offer",
  "sku": "$sku",
  "message": "$message",
  "skip_category": "$skip_category",
  "is_critical": $is_critical,
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
}
EOF
    done < "$skip_log" | jq -s '.' > "$tmp_sk" 2>/dev/null || echo "[]" > "$tmp_sk"
    
    jq --slurpfile arr "$tmp_sk" '.skips = $arr[0]' "$out" > "$out.tmp" && mv "$out.tmp" "$out"
    rm -f "$tmp_sk"
  fi

  # ===== MEJORA: Agregar estadísticas de resumen =====
  jq '.summary_stats = {
    by_status: {},
    by_series: {},  
    by_type: {},
    by_failure_category: {},
    by_skip_category: {},
    critical_skips: (.skips | map(select(.is_critical // false)) | length),
    policy_failures: 0,
    most_common_failures: []
  }' "$out" > "$out.tmp" && mv "$out.tmp" "$out" || true

  # Metrics per vm/test + vm rollup (código existente mejorado)
  if [ -d artifacts ]; then
    mapfile -t VMS < <(find artifacts -maxdepth 1 -mindepth 1 -type d -printf "%f\n" 2>/dev/null | sort)
    metrics_array="[]"
    vm_rollup="{}"
    
    for vm in "${VMS[@]}"; do
      [ -d "artifacts/$vm" ] || continue
      mapfile -t TESTS < <(find "artifacts/$vm" -maxdepth 1 -mindepth 1 -type d -printf "%f\n" 2>/dev/null | sort)
      merged='{}'
      
      for t in "${TESTS[@]}"; do
        f="artifacts/$vm/$t/stdout.log"
        m=$(collect_metrics_from_stdout "$f")
        
        # Agregar metadata del test
        m_with_meta=$(jq -cn --argjson m "$m" --arg test "$t" --arg vm "$vm" \
          '$m + {_test_name: $test, _vm_name: $vm, _timestamp: now | strftime("%Y-%m-%dT%H:%M:%SZ")}')
        
        metrics_array=$(jq -cn --arg vm "$vm" --arg t "$t" --argjson m "$m_with_meta" \
          --argjson arr "${metrics_array:-[]}" '$arr + [{vm:$vm, test:$t, metrics:$m}]')
        merged=$(jq -cn --argjson A "$merged" --argjson B "$m" '$A + $B')
      done
      
      # Agregar métricas derivadas por VM
      vm_metrics_count=$(echo "$merged" | jq 'keys | length')
      merged=$(jq -cn --argjson M "$merged" --argjson count "$vm_metrics_count" \
        '$M + {_metrics_count: $count, _vm_completion_time: now | strftime("%Y-%m-%dT%H:%M:%SZ")}')
      
      vm_rollup=$(jq -cn --arg vm "$vm" --argjson R "${vm_rollup:-{}}" --argjson M "$merged" \
        '$R + {($vm): $M}')
    done
    
    local tmp="$(mktemp)"
    jq --argjson arr "${metrics_array:-[]}" '.metrics = $arr' "$out" > "$tmp" && mv "$tmp" "$out"
    tmp="$(mktemp)"
    jq --argjson vr  "${vm_rollup:-{}}"  '.vm_rollup = $vr' "$out" > "$tmp" && mv "$tmp" "$out"
  fi
  
  # ===== MEJORA: Agregar información de rendimiento y duración =====
  if [ -f "$results_log" ]; then
    local tmp_perf="$(mktemp)"
    jq '
      .performance_stats = {
        total_vms: (.vm_rollup | keys | length),
        avg_metrics_per_vm: (
          if (.vm_rollup | keys | length) > 0 then
            (.vm_rollup | [.[] | ._metrics_count // 0] | add / length)
          else 0 end
        ),
        test_distribution: (.results | group_by(.test) | map({test: .[0].test, count: length}) | from_entries),
        completion_rate: (
          if .totals.total > 0 then
            ((.totals.GOOD + .totals.BAD) / .totals.total * 100 | floor)
          else 0 end
        )
      }
    ' "$out" > "$tmp_perf" && mv "$tmp_perf" "$out"
  fi
  
  # Validar el JSON final
  if ! jq empty "$out" 2>/dev/null; then
    warn "Generated JSON is invalid. Creating minimal fallback."
    jq -n --arg ts "$(now_utc)" --arg error "JSON generation failed" '
      {
        _comments: "Fallback summary due to JSON generation error",
        run: {timestamp: $ts},
        error: $error,
        totals: {GOOD: 0, BAD: 0, SKIP: 0}
      }
    ' > "$out"
  fi
}